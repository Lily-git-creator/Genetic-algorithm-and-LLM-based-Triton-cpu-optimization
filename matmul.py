import torch
import triton
import triton.language as tl
import time

# Triton çŸ©é˜µä¹˜æ³•å†…æ ¸ (ä¼˜åŒ–å¹³é“ºç‰ˆæœ¬)
@triton.jit
def matmul_kernel(
    # æŒ‡å‘è¾“å…¥è¾“å‡ºçŸ©é˜µæ•°æ®å—çš„æŒ‡é’ˆ
    a_ptr, b_ptr, c_ptr,
    # çŸ©é˜µç»´åº¦
    M, N, K,
    # æ­¥é•¿ï¼ˆç›¸é‚»è¡Œ/åˆ—ä¹‹é—´çš„å†…å­˜è·¨åº¦ï¼‰
    stride_am, stride_ak,  # A çš„æ­¥é•¿ (M, K)
    stride_bk, stride_bn,  # B çš„æ­¥é•¿ (K, N)
    stride_cm, stride_cn,  # C çš„æ­¥é•¿ (M, N)
    # å¹³é“ºå¤§å° (Tile Size) - ç¼–è¯‘å™¨å¸¸é‡ï¼Œç”¨äºä¼˜åŒ–
    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,
):
    """
    è®¡ç®— C = A @ B çš„ Triton å†…æ ¸ã€‚
    ä½¿ç”¨äºŒç»´ç½‘æ ¼å¸ƒå±€å’ŒåŒé‡å¾ªç¯å¹³é“ºä»¥ä¼˜åŒ– CPU ç¼“å­˜åˆ©ç”¨ç‡ã€‚
    """
    # å½“å‰ç¨‹åºè´Ÿè´£è®¡ç®—è¾“å‡ºçŸ©é˜µ C ä¸­çš„å“ªä¸ªåˆ†å— (pid_m, pid_n)
    pid_m = tl.program_id(axis=0)
    pid_n = tl.program_id(axis=1)

    # è®¡ç®—å½“å‰åˆ†å—åœ¨ A å’Œ B ä¸­çš„èµ·å§‹å†…å­˜åç§»
    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    
    # åˆå§‹åŒ–ç´¯åŠ å™¨ï¼ˆå­˜æ”¾å½“å‰åˆ†å—çš„éƒ¨åˆ†å’Œï¼‰
    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
    
    # åŒé‡å¾ªç¯å¹³é“º: æ²¿ K ç»´åº¦åˆ†å—åŠ è½½å’Œè®¡ç®—ï¼Œæé«˜ç¼“å­˜å‘½ä¸­ç‡
    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        # ä» A åŠ è½½å½“å‰å— (BLOCK_SIZE_M x BLOCK_SIZE_K)
        a = tl.load(
            a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak,
            mask=(offs_am[:, None] < M) & (offs_k[None, :] < K),
            other=0.0,
        )
        # ä» B åŠ è½½å½“å‰å— (BLOCK_SIZE_K x BLOCK_SIZE_N)
        b = tl.load(
            b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn,
            mask=(offs_k[:, None] < K) & (offs_bn[None, :] < N),
            other=0.0,
        )
        # çŸ©é˜µä¹˜ç´¯åŠ : [BLOCK_SIZE_M, BLOCK_SIZE_N] += [BLOCK_SIZE_M, BLOCK_SIZE_K] @ [BLOCK_SIZE_K, BLOCK_SIZE_N]
        accumulator += tl.dot(a, b)
        # ç§»åŠ¨åˆ° K ç»´åº¦çš„ä¸‹ä¸€ä¸ªåˆ†å—
        offs_k += BLOCK_SIZE_K

    # å°†æœ€ç»ˆç»“æœå†™å› C
    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
    tl.store(c_ptrs, accumulator, mask=c_mask)

def triton_matmul(a, b):
    """
    ä½¿ç”¨ Triton å†…æ ¸æ‰§è¡ŒçŸ©é˜µä¹˜æ³• a @ bã€‚
    å‚æ•°:
        a, b: 2D PyTorch å¼ é‡ (CPU)
    è¿”å›:
        c: 2D PyTorch å¼ é‡ (CPU), ç»“æœä¸º a @ b
    """
    # æ£€æŸ¥è¾“å…¥
    assert a.dim() == 2 and b.dim() == 2, "è¾“å…¥å¿…é¡»æ˜¯äºŒç»´å¼ é‡"
    assert a.shape[1] == b.shape[0], f"çŸ©é˜µç»´åº¦ä¸åŒ¹é…: {a.shape} @ {b.shape}"
    
    M, K = a.shape
    K_check, N = b.shape
    assert K == K_check, "å†…éƒ¨ç»´åº¦ K å¿…é¡»ç›¸ç­‰"
    
    # é¢„åˆ†é…è¾“å‡ºå¼ é‡
    c = torch.zeros((M, N), device=a.device, dtype=a.dtype)
    
    # å®šä¹‰ç½‘æ ¼å’Œåˆ†å—å¤§å° (å¯æ ¹æ®ç¡¬ä»¶å¾®è°ƒ)
    BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K = 64, 64, 32  # å¹³é“ºå‚æ•°
    
    # è®¡ç®—äºŒç»´ç½‘æ ¼å¤§å° (æ¯ä¸ªè¾“å‡ºåˆ†å—ä¸€ä¸ªçº¿ç¨‹)
    grid = (
        triton.cdiv(M, BLOCK_SIZE_M),
        triton.cdiv(N, BLOCK_SIZE_N),
    )
    
    # å¯åŠ¨å†…æ ¸
    matmul_kernel[grid](
        a, b, c,
        M, N, K,
        a.stride(0), a.stride(1),
        b.stride(0), b.stride(1),
        c.stride(0), c.stride(1),
        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,
    )
    return c

def test_correctness_and_performance():
    """
    ä¸»æµ‹è¯•å‡½æ•°: æ¯”è¾ƒ Triton ä¸ PyTorch å®ç°çš„æ­£ç¡®æ€§å’Œæ€§èƒ½ã€‚
    """
    print("ğŸ” å¼€å§‹ Triton-CPU çŸ©é˜µä¹˜æ³•æµ‹è¯•")
    print("-" * 50)
    
    # æµ‹è¯•ç”¨ä¾‹ (å¯æ ¹æ®éœ€è¦è°ƒæ•´å¤§å°)
    test_cases = [
        (128, 256, 512),   # å°è§„æ¨¡
        (512, 1024, 2048), # ä¸­ç­‰è§„æ¨¡
        (1024, 2048, 4096),# å¤§è§„æ¨¡
    ]
    
    for idx, (M, N, K) in enumerate(test_cases):
        print(f"\næµ‹è¯•ç”¨ä¾‹ {idx+1}: A[{M}x{K}] @ B[{K}x{N}] = C[{M}x{N}]")
        
        # ç”Ÿæˆéšæœºæµ‹è¯•æ•°æ® (CPU)
        torch.manual_seed(42)
        a = torch.randn((M, K), device='cpu', dtype=torch.float32)
        b = torch.randn((K, N), device='cpu', dtype=torch.float32)
        
        # **1. ä½¿ç”¨ Triton è®¡ç®—**
        start_time = time.perf_counter()
        c_triton = triton_matmul(a, b)
        triton_time = time.perf_counter() - start_time
        
        # **2. ä½¿ç”¨ PyTorch (å‚è€ƒå®ç°) è®¡ç®—**
        start_time = time.perf_counter()
        c_ref = torch.matmul(a, b)  # æˆ– a @ b
        torch_time = time.perf_counter() - start_time
        
        # **3. æ­£ç¡®æ€§éªŒè¯: è®¡ç®—æœ€å¤§ç»å¯¹è¯¯å·®**
        max_abs_error = torch.max(torch.abs(c_triton - c_ref)).item()
        # ç›¸å¯¹è¯¯å·®å®¹é™ (è€ƒè™‘æµ®ç‚¹è®¡ç®—è¯¯å·®)
        rtol, atol = 1e-3, 1e-3
        is_correct = torch.allclose(c_triton, c_ref, rtol=rtol, atol=atol)
        
        # **4. æ€§èƒ½æ¯”è¾ƒ**
        # è®¡ç®—ç†è®ºæµ®ç‚¹è¿ç®—æ¬¡æ•° (FLOPs)
        flops = 2.0 * M * N * K  # çŸ©é˜µä¹˜æ³•æµ®ç‚¹è¿ç®—æ•°
        triton_gflops = (flops / triton_time) / 1e9
        torch_gflops = (flops / torch_time) / 1e9
        speedup = torch_time / triton_time
        
        # **5. æ‰“å°ç»“æœ**
        print(f"   æ­£ç¡®æ€§: {'âœ… PASS' if is_correct else 'âŒ FAIL'}")
        if not is_correct:
            print(f"      æœ€å¤§ç»å¯¹è¯¯å·®: {max_abs_error:.2e}")
            print(f"      å®¹é™ (rtol={rtol}, atol={atol})")
        print(f"   æ€§èƒ½å¯¹æ¯”:")
        print(f"     - Triton: {triton_time*1000:.2f} ms, {triton_gflops:.2f} GFLOP/s")
        print(f"     - PyTorch: {torch_time*1000:.2f} ms, {torch_gflops:.2f} GFLOP/s")
        print(f"     åŠ é€Ÿæ¯”: {speedup:.2f}x {'(Tritonæ›´å¿«)' if speedup > 1.0 else '(PyTorchæ›´å¿«)'}")
    
    print("\n" + "=" * 50)
    print("æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    # å¯é€‰ï¼šé¢„çƒ­è¿è¡Œï¼ˆé¿å…é¦–æ¬¡å¯åŠ¨å¼€é”€å½±å“è®¡æ—¶ï¼‰
    print("é¢„çƒ­è¿è¡Œ...")
    warmup_a = torch.randn((64, 64), device='cpu')
    warmup_b = torch.randn((64, 64), device='cpu')
    _ = triton_matmul(warmup_a, warmup_b)
    _ = torch.matmul(warmup_a, warmup_b)
    
    # è¿è¡Œä¸»æµ‹è¯•
    test_correctness_and_performance()

'''
python ./matmul.py
é¢„çƒ­è¿è¡Œ...
ğŸ” å¼€å§‹ Triton-CPU çŸ©é˜µä¹˜æ³•æµ‹è¯•
--------------------------------------------------

æµ‹è¯•ç”¨ä¾‹ 1: A[128x512] @ B[512x256] = C[128x256]
   æ­£ç¡®æ€§: âœ… PASS
   æ€§èƒ½å¯¹æ¯”:
     - Triton: 5.23 ms, 6.41 GFLOP/s
     - PyTorch: 50.44 ms, 0.67 GFLOP/s
     åŠ é€Ÿæ¯”: 9.64x (Tritonæ›´å¿«)

æµ‹è¯•ç”¨ä¾‹ 2: A[512x2048] @ B[2048x1024] = C[512x1024]
   æ­£ç¡®æ€§: âœ… PASS
   æ€§èƒ½å¯¹æ¯”:
     - Triton: 281.43 ms, 7.63 GFLOP/s
     - PyTorch: 2.98 ms, 720.99 GFLOP/s
     åŠ é€Ÿæ¯”: 0.01x (PyTorchæ›´å¿«)

æµ‹è¯•ç”¨ä¾‹ 3: A[1024x4096] @ B[4096x2048] = C[1024x2048]
   æ­£ç¡®æ€§: âœ… PASS
   æ€§èƒ½å¯¹æ¯”:
     - Triton: 2312.28 ms, 7.43 GFLOP/s
     - PyTorch: 16.26 ms, 1056.26 GFLOP/s
     åŠ é€Ÿæ¯”: 0.01x (PyTorchæ›´å¿«)

==================================================
æµ‹è¯•å®Œæˆï¼
'''